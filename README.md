Assignments and Default Project for Course: CS224N_2021: Natural Language Processing with Deep Learning

Each assignment (a1â€“a5) contains two parts: coding and written.

Written part: Analysis and insights about the model and the problem

Coding part: Implement fundamental blocks of neural models taught in the videos from starter code.

## a1: Assignment 1: Exploring Word Vectors

Written: Analysis and reasoning about the word embeddings from the GloVe model and co-occurrence-based word vectors.

Coding: Rebuild co-occurrence-based word vectors and explore the word embeddings.

## a2: Assignment 2: Understanding Word2Vec

Written: Gradient computations and reporting the results of the Word2Vec model.

Coding: Rebuild the Word2Vec model and its variation: the Skip-gram model in Word2Vec (naiveSoftmaxLossAndGradient and negSamplingLossAndGradient approach).

## a3: Assignment 3: Dependency Parsing

Written: Insights and tips about training neural models; understand the operation of the dependency parser and errors from the neural dependency parsing model.

Coding: Rebuild the neural dependency parsing model: parser model (deep learning model for predicting the next transition)

, parser transitions: operate transitions (RIGHT ARC, LEFT ARC, ADD).
## a4: Assignment 4: Neural Machine Translation with RNNs

Written: Insights about different implementations of attention operations and masked parts in the sentence; 

detection and correction of errors from the model; understanding BLEU.

Coding: Implement fundamental building blocks of the Seq2Seq model from the starter code.

## Key skills

+ PyTorch
+ Hugging Face
+ LaTeX
  
Updated on 07/10/2023

## Future work

+ a5: Assignment 5 (Self-Attention, Transformers, Pretraining): Plan completed: 14/10/2023
+ Default Project: Question Answering: Plan completed: 10/11/2023
