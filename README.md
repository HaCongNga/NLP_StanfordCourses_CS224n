Assignments and Default Project for Course: CS224N_2021: Natural Language Processing with Deep Learning

Each assignment (a1â€“a5) contains two parts: coding and written.

Written part: Analysis and insights about the model and the problem

Coding part: Implement fundamental blocks of neural models taught in the videos from starter code.

## a1: Assignment 1: Exploring Word Vectors

Written: Analysis and reasoning about the word embeddings from the GloVe model and co-occurrence-based word vectors.

Coding: Rebuild co-occurrence-based word vectors and explore the word embeddings.

## a2: Assignment 2: Understanding Word2Vec

Written: Gradient computations and reporting the results of the Word2Vec model.

Coding: Rebuild the Word2Vec model and its variation

## a3: Assignment 3: Dependency Parsing

Written: Insights and tips about training neural models; understand the operation of the dependency parser and errors from the neural dependency parsing model.

Coding: Rebuild the neural dependency parsing model: parser model (deep learning model for predicting the next transition)

, parser transitions: operate transitions (RIGHT ARC, LEFT ARC, ADD).
## a4: Assignment 4: Neural Machine Translation with RNNs

Written: Insights about different implementations of attention operations and masked parts in the sentence; 

detection and correction of errors from the model; understanding BLEU.

Coding: Implement fundamental building blocks of the Seq2Seq model.
## a5 : Assignment 5 : Self-Attention, Transformers, and Pretraining
Written: + Gaining mathematical intuitions about the advantages of multi-headed attention over single one. 
         + Empirical insights about the benefits of pretrained models.

Coding: Reimplement Self-attention, pretraining and fine-tuning LLM, minGPT.

## Project : Default Project : BERT and downstream tasks

## Key skills
+ Mathematics (Linear Algebra, Statistics, Calculus)
+ PyTorch
+ Hugging Face
+ LaTeX

